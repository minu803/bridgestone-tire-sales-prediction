{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df289683",
   "metadata": {},
   "source": [
    "# Drop duplicates from sales data\n",
    "\n",
    "The raw sales data contains an estimated 10% of duplicate values. This notebook will process the data to remove them, and create new datasets for future use in EDA and modeling. Runtimes are also taken and printed to test and track efficiency. \n",
    "\n",
    "The output from this notebook are 43 csv files, which are stored in the team7/removed-duplicates directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbd132",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2f3daefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4380b8f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "91f700e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STORE_ID',\n",
       " 'TRAN_ID',\n",
       " 'DATE',\n",
       " 'ARTICLE_ID',\n",
       " 'INDIV_ID',\n",
       " 'VEHICLE_ID',\n",
       " 'UNITS',\n",
       " 'SALES']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names only\n",
    "header_only = pd.read_csv(\"sales_20180228.csv\", sep=\"|\", nrows=0)\n",
    "cols = header_only.columns.to_list()\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c5d5292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs52/data/p_dsi/teams2023/bridgestone_data/data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000000, 8)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nrows = 1M for testing\n",
    "%cd /data/p_dsi/teams2023/bridgestone_data/data\n",
    "\n",
    "dat_test_tiny = pd.read_csv(\"sales_20180228.csv\", sep=\"|\", nrows=1000000, parse_dates=[\"DATE\"])\n",
    "dat_test_tiny.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6c03c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs52/data/p_dsi/teams2023/bridgestone_data/data\n",
      "CPU times: user 6.21 s, sys: 602 ms, total: 6.81 s\n",
      "Wall time: 1min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13247533, 8)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%cd /data/p_dsi/teams2023/bridgestone_data/data\n",
    "\n",
    "dat_test_full = pd.read_csv(\"sales_20180228.csv\", sep=\"|\", parse_dates=[\"DATE\"])\n",
    "dat_test_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "68aa726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.22 s, sys: 605 ms, total: 6.82 s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dat_test_full = pd.read_csv(\"sales_20180228.csv\", sep=\"|\", parse_dates=[\"DATE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6841511a",
   "metadata": {},
   "source": [
    "## Custom Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01deb6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates_test(data_raw, data_dropped):\n",
    "    '''\n",
    "    Check whether duplicates are properly removed using drop_duplicates out-of-the-box on the raw sales data.\n",
    "    \n",
    "    data_raw = original sales dataframe\n",
    "    data_dropped = dataframe of month of sales data after dropping duplicates\n",
    "    '''\n",
    "    # group data and add count column\n",
    "    cols = data_raw.columns.tolist()\n",
    "    temp = data_raw.groupby(cols).size().reset_index(name=\"count\")\n",
    "    target_nrows = temp.shape[0]   # target = nrows after grouping\n",
    "    \n",
    "    # run test:\n",
    "    print(\". . . \\n\")\n",
    "    print(\"Results from duplicates_test \\n\")\n",
    "    if data_dropped.shape[0] == target_nrows: print(\"PASS\")\n",
    "    else: print(\"FAIL: nrows do not match\")\n",
    "    \n",
    "    # sanity check - print descriptive info\n",
    "    print(\"Original nrows: \", data_raw.shape[0])\n",
    "    print(\"Target nrows: \", target_nrows) \n",
    "    print(\". . . \\n\")\n",
    "    print(\"Nrows after drop_duplicates: \", data_dropped.shape[0])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "924f4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates_big_data(bigdata, cols, filename): \n",
    "    '''\n",
    "    Given a big data dataframe, removes duplicates. Returns 'new_df'.\n",
    "    \n",
    "    bigdata = pandas dataframe of the big dataset\n",
    "    cols = list of sales columns\n",
    "    filename = passed in from looping over csv_list\n",
    "    '''\n",
    "    temp = bigdata.groupby(cols).size().reset_index(name=\"count\")\n",
    "    temp_nonduplicate_obs = temp.query(\"count == 1\").drop([\"count\"], axis=1)\n",
    "    temp_duplicate_obs = temp.query(\"count > 1\").drop([\"count\"], axis=1)\n",
    "    \n",
    "    # df with duplicates removed\n",
    "    temp_drop_duplicates = temp_duplicate_obs.drop_duplicates()\n",
    "    # concat dfs with distinct obs\n",
    "    new_df = pd.concat([temp_nonduplicate_obs, temp_drop_duplicates], axis=0)\n",
    "\n",
    "# print message for testing only:    \n",
    "#     print(\". . . \\n\")\n",
    "#     print(\"Results from drop_duplicates_big_data \\n\")\n",
    "#     print(\"Shape of new df (after dropping duplicates): \", new_df.shape)\n",
    "#     print(\". . . \\n\")\n",
    "    \n",
    "    return new_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05f34a",
   "metadata": {},
   "source": [
    "### Test drop_duplicates_big_data on 1M rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b40d8a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      "\n",
      "Results from drop_duplicates_big_data: \n",
      "\n",
      "Shape of new df (after dropping duplicates):  (927393, 8)\n",
      ". . . \n",
      "\n",
      ". . . \n",
      "\n",
      "Results from duplicates_test: \n",
      "\n",
      "PASS\n",
      "Original nrows:  1000000\n",
      "Target nrows:  927393\n",
      ". . . \n",
      "\n",
      "Nrows after drop_duplicates:  927393\n",
      "CPU times: user 1.14 s, sys: 0 ns, total: 1.14 s\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "drop_test = drop_duplicates_big_data(dat_test_tiny)\n",
    "\n",
    "duplicates_test(data_raw=dat_test_tiny, data_dropped=drop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f7d2b",
   "metadata": {},
   "source": [
    "## Test drop_duplicates_big_data on one sales file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde5bd94",
   "metadata": {},
   "source": [
    "Test on pre-loaded dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c73a2454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      "\n",
      "Results from drop_duplicates_big_data: \n",
      "\n",
      "Shape of new df (after dropping duplicates):  (11897920, 8)\n",
      ". . . \n",
      "\n",
      ". . . \n",
      "\n",
      "Results from duplicates_test: \n",
      "\n",
      "PASS\n",
      "Original nrows:  13247533\n",
      "Target nrows:  11897920\n",
      ". . . \n",
      "\n",
      "Nrows after drop_duplicates:  11897920\n",
      "CPU times: user 38.6 s, sys: 2.9 s, total: 41.5 s\n",
      "Wall time: 41.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "drop_test = drop_duplicates_big_data(dat_test_full)\n",
    "\n",
    "duplicates_test(data_raw=dat_test_full, data_dropped=drop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78785a",
   "metadata": {},
   "source": [
    "Now write for loop to operate across all 43 sales csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba7c87c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 s, sys: 2.28 s, total: 24.8 s\n",
      "Wall time: 24.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11897920"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cols = drop_test.columns.to_list()\n",
    "drop_test.sort_values(cols).groupby(cols).size().reset_index(name=\"count\").query(\"count == 1\").shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7ffd7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs52/data/p_dsi/teams2023/team7\n"
     ]
    }
   ],
   "source": [
    "%cd /data/p_dsi/teams2023/team7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f4309d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = []\n",
    "for file in os.listdir(\"/data/p_dsi/teams2023/bridgestone_data/data\"):\n",
    "    if file.startswith('sales_2'):\n",
    "        csv_files.append(file)\n",
    "\n",
    "if len(csv_files) != 43: print(\"ERROR - csv_files length = \", len(csv_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d11deb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      "\n",
      "Results from drop_duplicates_big_data \n",
      "\n",
      "Shape of new df (after dropping duplicates):  (13640515, 8)\n",
      ". . . \n",
      "\n",
      "fParsing complete: #{count}, {filename}\n",
      "CPU times: user 2min 17s, sys: 3.45 s, total: 2min 20s\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# store col names to be accessed in the loop\n",
    "cols = dat_test_tiny.columns.to_list()\n",
    "# loop counter variable\n",
    "count = 1\n",
    "\n",
    "for filename in csv_files:\n",
    "    # store in and out file paths as variables\n",
    "    path_in = \"/data/p_dsi/teams2023/bridgestone_data/data/\" + filename\n",
    "    path_out = \"/data/p_dsi/teams2023/team7/remove-duplicates/\" + filename\n",
    "    \n",
    "    # read in one sales csv as panda dataframe\n",
    "    df_read = pd.read_csv(path_in, sep = \"|\", parse_dates=[\"DATE\"])\n",
    "    \n",
    "    # drop duplicates and store in new df\n",
    "    temp = drop_duplicates_big_data(bigdata=df_read, cols=cols)\n",
    "    \n",
    "    # write new df to csv\n",
    "    temp.to_csv(path_out, index=False)\n",
    "    \n",
    "    # status message with loop count\n",
    "    print(\"fParsing complete: #{count}, {filename}\")\n",
    "    count += 1\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3c4ac6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1c31de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_opts = csv.ParseOptions(delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cf83b19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.62 s, sys: 1.05 s, total: 6.67 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pa = csv.read_csv(\"sales_20180228.csv\", parse_options=parse_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c6190554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 540 ms, sys: 333 ms, total: 873 ms\n",
      "Wall time: 134 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_convert_pd = test_pa.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c26eab",
   "metadata": {},
   "source": [
    "Test run the for loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aa950b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing complete: #1, sales_20180731.csv\n",
      "{'sales_20180731.csv': 13640515}\n",
      "CPU times: user 1min 27s, sys: 4.78 s, total: 1min 32s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set options to read csv w/pyarrow\n",
    "parse_opts = csv.ParseOptions(delimiter=\"|\")\n",
    "read_opts = csv.ReadOptions(use_threads=4)\n",
    "# loop counter variable\n",
    "count = 1\n",
    "# dict to store nrows of each iteration\n",
    "results_nrows = {}\n",
    "\n",
    "for filename in csv_files:\n",
    "    # store in and out file paths as variables\n",
    "    path_in = \"/data/p_dsi/teams2023/bridgestone_data/data/\" + filename\n",
    "    path_out = \"/data/p_dsi/teams2023/team7/remove-duplicates/\" + filename\n",
    "    \n",
    "    # read in one sales csv as Arrow table\n",
    "    arrow_table = csv.read_csv(path_in, parse_options=parse_opts)\n",
    "    # convert Arrow table to pandas df\n",
    "    df = arrow_table.to_pandas()\n",
    "    \n",
    "    # drop duplicates and store in new df\n",
    "    temp = drop_duplicates_big_data(bigdata=df, cols=cols, filename=filename)\n",
    "    \n",
    "    # store the nrows from each df in a dict\n",
    "    results_nrows.update({f\"{filename}\": temp.shape[0]})\n",
    "    \n",
    "    # write new df to csv\n",
    "    temp.to_csv(path_out, index=False)\n",
    "    \n",
    "    # status message with loop count\n",
    "    print(f\"Parsing complete: #{count}, {filename}\")\n",
    "    count += 1\n",
    "    print(results_nrows)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "95822286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      "\n",
      "Results from duplicates_test: \n",
      "\n",
      "PASS\n",
      "Original nrows:  15197671\n",
      "Target nrows:  13640515\n",
      ". . . \n",
      "\n",
      "Nrows after drop_duplicates:  13640515\n",
      "CPU times: user 20.7 s, sys: 1.32 s, total: 22.1 s\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sanity check on above loop test\n",
    "duplicates_test(data_raw=df, data_dropped=temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1398f2e",
   "metadata": {},
   "source": [
    "Run full loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "17c8b4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: File no. 1, File name: sales_20180731.csv\n",
      "Processed: File no. 2, File name: sales_20150731.csv\n",
      "Processed: File no. 3, File name: sales_20170831.csv\n",
      "Processed: File no. 4, File name: sales_20170731.csv\n",
      "Processed: File no. 5, File name: sales_20150531.csv\n",
      "Processed: File no. 6, File name: sales_20160131.csv\n",
      "Processed: File no. 7, File name: sales_20151130.csv\n",
      "Processed: File no. 8, File name: sales_20170531.csv\n",
      "Processed: File no. 9, File name: sales_20160331.csv\n",
      "Processed: File no. 10, File name: sales_20170228.csv\n",
      "Processed: File no. 11, File name: sales_20160930.csv\n",
      "Processed: File no. 12, File name: sales_20170131.csv\n",
      "Processed: File no. 13, File name: sales_20180831.csv\n",
      "Processed: File no. 14, File name: sales_20160430.csv\n",
      "Processed: File no. 15, File name: sales_20170331.csv\n",
      "Processed: File no. 16, File name: sales_20171031.csv\n",
      "Processed: File no. 17, File name: sales_20180531.csv\n",
      "Processed: File no. 18, File name: sales_20160831.csv\n",
      "Processed: File no. 19, File name: sales_20170630.csv\n",
      "Processed: File no. 20, File name: sales_20161031.csv\n",
      "Processed: File no. 21, File name: sales_20160630.csv\n",
      "Processed: File no. 22, File name: sales_20170930.csv\n",
      "Processed: File no. 23, File name: sales_20180430.csv\n",
      "Processed: File no. 24, File name: sales_20161231.csv\n",
      "Processed: File no. 25, File name: sales_20171231.csv\n",
      "Processed: File no. 26, File name: sales_20180630.csv\n",
      "Processed: File no. 27, File name: sales_20180228.csv\n",
      "Processed: File no. 28, File name: sales_20180131.csv\n",
      "Processed: File no. 29, File name: sales_20160731.csv\n",
      "Processed: File no. 30, File name: sales_20170430.csv\n",
      "Processed: File no. 31, File name: sales_20150430.csv\n",
      "Processed: File no. 32, File name: sales_20181031.csv\n",
      "Processed: File no. 33, File name: sales_20161130.csv\n",
      "Processed: File no. 34, File name: sales_20180930.csv\n",
      "Processed: File no. 35, File name: sales_20150930.csv\n",
      "Processed: File no. 36, File name: sales_20150831.csv\n",
      "Processed: File no. 37, File name: sales_20160531.csv\n",
      "Processed: File no. 38, File name: sales_20151031.csv\n",
      "Processed: File no. 39, File name: sales_20180331.csv\n",
      "Processed: File no. 40, File name: sales_20160229.csv\n",
      "Processed: File no. 41, File name: sales_20151231.csv\n",
      "Processed: File no. 42, File name: sales_20150630.csv\n",
      "Processed: File no. 43, File name: sales_20171130.csv\n",
      "Parsing complete, check directory for new csvs.\n",
      "CPU times: user 1h 1min 21s, sys: 3min 38s, total: 1h 4min 59s\n",
      "Wall time: 3h 57min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set options to read csv w/pyarrow\n",
    "parse_opts = csv.ParseOptions(delimiter=\"|\")\n",
    "read_opts = csv.ReadOptions(use_threads=4)\n",
    "# loop counter variable\n",
    "count = 1\n",
    "# dict to store nrows of each iteration\n",
    "results_nrows = {}\n",
    "\n",
    "for filename in csv_files:\n",
    "    # store in and out file paths as variables\n",
    "    path_in = \"/data/p_dsi/teams2023/bridgestone_data/data/\" + filename\n",
    "    path_out = \"/data/p_dsi/teams2023/team7/remove-duplicates/\" + filename\n",
    "    \n",
    "    # read in one sales csv as Arrow table\n",
    "    arrow_table = csv.read_csv(path_in, parse_options=parse_opts)\n",
    "    # convert Arrow table to pandas df\n",
    "    df = arrow_table.to_pandas()\n",
    "    \n",
    "    # drop duplicates and store in new df\n",
    "    temp = drop_duplicates_big_data(bigdata=df, cols=cols, filename=filename)\n",
    "    \n",
    "    # write new df to csv\n",
    "    temp.to_csv(path_out, index=False)\n",
    "    \n",
    "    # status message with loop count\n",
    "    print(f\"Processed: File no. {count}, File name: {filename}\")\n",
    "    count += 1\n",
    "print(\"Parsing complete, check directory for new csvs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c4e15",
   "metadata": {},
   "source": [
    "#### Check one of the new files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "59190b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE_ID</th>\n",
       "      <th>TRAN_ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ARTICLE_ID</th>\n",
       "      <th>INDIV_ID</th>\n",
       "      <th>VEHICLE_ID</th>\n",
       "      <th>UNITS</th>\n",
       "      <th>SALES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>991724790</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>7003186</td>\n",
       "      <td>266764026.0</td>\n",
       "      <td>933927006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>991724790</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>7003189</td>\n",
       "      <td>266764026.0</td>\n",
       "      <td>933927006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>991724790</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>7003348</td>\n",
       "      <td>266764026.0</td>\n",
       "      <td>933927006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>991724790</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>7046930</td>\n",
       "      <td>266764026.0</td>\n",
       "      <td>933927006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>991726170</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>7004228</td>\n",
       "      <td>277240279.0</td>\n",
       "      <td>965350926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>991726170</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>7005229</td>\n",
       "      <td>277240279.0</td>\n",
       "      <td>965350926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27</td>\n",
       "      <td>991726170</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>7005537</td>\n",
       "      <td>277240279.0</td>\n",
       "      <td>965350926</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>991726170</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>7008209</td>\n",
       "      <td>277240279.0</td>\n",
       "      <td>965350926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>991726170</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>7008406</td>\n",
       "      <td>277240279.0</td>\n",
       "      <td>965350926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27</td>\n",
       "      <td>991726170</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>7008409</td>\n",
       "      <td>277240279.0</td>\n",
       "      <td>965350926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STORE_ID    TRAN_ID        DATE  ARTICLE_ID     INDIV_ID  VEHICLE_ID  \\\n",
       "0        27  991724790  2018-10-30     7003186  266764026.0   933927006   \n",
       "1        27  991724790  2018-10-30     7003189  266764026.0   933927006   \n",
       "2        27  991724790  2018-10-30     7003348  266764026.0   933927006   \n",
       "3        27  991724790  2018-10-30     7046930  266764026.0   933927006   \n",
       "4        27  991726170  2018-10-01     7004228  277240279.0   965350926   \n",
       "5        27  991726170  2018-10-01     7005229  277240279.0   965350926   \n",
       "6        27  991726170  2018-10-01     7005537  277240279.0   965350926   \n",
       "7        27  991726170  2018-10-01     7008209  277240279.0   965350926   \n",
       "8        27  991726170  2018-10-01     7008406  277240279.0   965350926   \n",
       "9        27  991726170  2018-10-01     7008409  277240279.0   965350926   \n",
       "\n",
       "   UNITS   SALES  \n",
       "0    0.0    0.00  \n",
       "1    1.0    0.00  \n",
       "2    0.0    0.00  \n",
       "3    0.0    0.00  \n",
       "4    0.0   20.00  \n",
       "5    0.0  199.99  \n",
       "6    1.0   29.99  \n",
       "7    0.0   20.00  \n",
       "8    0.0   17.99  \n",
       "9    0.0   17.99  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/data/p_dsi/teams2023/team7/remove-duplicates/sales_20181031.csv\", nrows=10)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
